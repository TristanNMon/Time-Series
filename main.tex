\documentclass[11pt]{article}
\usepackage{theme}
\usepackage{shortcuts}
% Document parameters
% Document title
\title{Assignment 1 (ML for TS) - MVA}
\author{
Jules Royer \email{jules.royer@etu.minesparis.psl.eu} \\ % student 1
Tristan Montalbetti \email{tristan.montalbetti@etu.minesparis.psl.eu} % student 2
}

\begin{document}
\maketitle

\section{Introduction}

\paragraph{Objective.} This assignment has three parts: questions about convolutional dictionary learning, spectral features, and a data study using the DTW. 

\paragraph{Warning and advice.} 
\begin{itemize}
    \item Use code from the tutorials as well as from other sources. Do not code yourself well-known procedures (e.g., cross-validation or k-means); use an existing implementation. 
    \item The associated notebook contains some hints and several helper functions.
    \item Be concise. Answers are not expected to be longer than a few sentences (omitting calculations).
\end{itemize}



\paragraph{Instructions.}
\begin{itemize}
    \item Fill in your names and emails at the top of the document.
    \item Hand in your report (one per pair of students) by Sunday 9\textsuperscript{th} November 23:59 PM.
    \item Rename your report and notebook as follows:\\ \texttt{FirstnameLastname1\_FirstnameLastname2.pdf} and\\ \texttt{FirstnameLastname1\_FirstnameLastname2.ipynb}.\\
    For instance, \texttt{LaurentOudre\_ValerioGuerrini.pdf}.
    \item Upload your report (PDF file) and notebook (IPYNB file) using this link: \footnotesize{LINK}.
\end{itemize}


\section{Convolution dictionary learning}

\begin{exercise}
Consider the following Lasso regression:
\begin{equation}\label{eq:lasso}
    \min_{\beta\in\RR^p} \frac{1}{2}\norm{y-X\beta}^2_2 \quad + \quad \lambda \norm{\beta}_1
\end{equation}
where $y\in\RR^n$ is the response vector, $X\in\RR^{n\times p}$ the design matrix, $\beta\in\RR^p$ the vector of regressors and $\lambda>0$ the smoothing parameter.

Show that there exists $\lambda_{\max}$ such that the minimizer of~\eqref{eq:lasso} is $\mathbf{0}_p$ (a $p$-dimensional vector of zeros) for any $\lambda > \lambda_{\max}$. 
\end{exercise}

\begin{solution}  % ANSWER HERE

\subsection*{1. The Primal Problem}
The Lasso optimization problem is given by:
\[
(P) \quad p^* = \min_{\beta \in \mathbb{R}^p} \quad \frac{1}{2}\|y - X\beta\|_2^2 + \lambda \|\beta\|_1
\]
This is an unconstrained convex optimization problem.

\subsection*{2. Equivalent Constrained Formulation}
To find the dual, we first introduce a new variable $z \in \mathbb{R}^n$ to represent the residual. We can rewrite the problem as an equivalent constrained optimization problem:
\[
(P') \quad p^* = \min_{\beta \in \mathbb{R}^p, z \in \mathbb{R}^n} \quad \frac{1}{2}\|z\|_2^2 + \lambda \|\beta\|_1
\]
\[
\text{subject to} \quad z + X\beta = y
\]

\subsection*{3. Strong Duality via Slater's Condition}
This new problem $(P')$ is a \textbf{convex optimization problem}:
\begin{itemize}
    \item The objective function $f_0(\beta, z) = \frac{1}{2}\|z\|_2^2 + \lambda \|\beta\|_1$ is a sum of two convex functions (the squared $L_2$-norm and the $L_1$-norm, with $\lambda > 0$), and is therefore convex.
    \item The constraint $h(\beta, z) = -z - X\beta + y = 0$ is an \textbf{affine} constraint.
\end{itemize}
For a convex optimization problem with only affine constraints, a refinement of \textbf{Slater's condition} states that strong duality holds as long as the problem is feasible.

The problem is clearly feasible; for example, one can choose $\beta = 0_p$ (the $p$-dimensional zero vector) and $z = y$. This satisfies the constraint.

Because strong duality holds, the optimal value of the primal ($p^*$) is equal to the optimal value of the dual ($d^*$), and the KKT conditions are necessary and sufficient for optimality.

\subsection*{4. The Lagrangian and Dual Function}
We introduce a Lagrange multiplier (dual variable) $\nu \in \mathbb{R}^n$ for the affine constraint. The Lagrangian $L$ is:
\[
L(\beta, z, \nu) = (\text{primal objective}) + \nu^T (\text{constraint})
\]
\[
L(\beta, z, \nu) = \frac{1}{2}\|z\|_2^2 + \lambda \|\beta\|_1 + \nu^T (y - X\beta - z)
\]
The Lagrange dual function $g(\nu)$ is the infimum of the Lagrangian over the primal variables ($\beta$ and $z$):
\[
g(\nu) = \inf_{\beta, z} L(\beta, z, \nu)
\]
We can group the terms for $\beta$ and $z$ and minimize them independently:
\[
g(\nu) = \inf_{z \in \mathbb{R}^n} \left( \frac{1}{2}\|z\|_2^2 - \nu^T z \right) + \inf_{\beta \in \mathbb{R}^p} \left( \lambda \|\beta\|_1 - \nu^T X\beta \right) + y^T\nu
\]
\textbf{Minimization w.r.t. $z$:}
The function $L_z = \frac{1}{2}\|z\|_2^2 - \nu^T z$ is convex and differentiable. We find the minimum by setting its gradient to zero:
\[
\nabla_z L_z = z - \nu = 0 \quad \implies \quad z^* = \nu
\]
The minimum value is $L_z(z^*) = \frac{1}{2}\|\nu\|_2^2 - \nu^T\nu = -\frac{1}{2}\|\nu\|_2^2$.

\textbf{Minimization w.r.t. $\beta$:}
This is the conjugate function of the $L_1$-norm. Let $v = X^T \nu$.
\[
\inf_{\beta \in \mathbb{R}^p} \left( \lambda \|\beta\|_1 - \nu^T X\beta \right) = -\sup_{ \beta \in \mathbb{R}^p} \left(  \left(\frac{1}{\lambda}X^T\nu\right)^T\lambda\beta -  \|\lambda\beta\|_1\right)
\]
Let's study the conjugate function of $L_1$-norm:

Let $x\in\mathbb{R}^n$. The $\ell_1$ norm is
\[
\|x\|_1 := \sum_{i=1}^n |x_i|.
\]
For a function $f:\mathbb{R}^n\to\mathbb{R}\cup\{+\infty\}$ the \emph{convex conjugate} (Fenchel conjugate) $f^*$ is defined by
\[
f^*(y) := \sup_{x\in\mathbb{R}^n}\{\langle y,x\rangle - f(x)\},\qquad y\in\mathbb{R}^n,
\]
where $\langle y,x\rangle = \sum_{i=1}^n y_i x_i$.

We will also use the dual norm fact: if $\|\cdot\|$ is a norm and $\|\cdot\|_*$ denotes its dual norm,
\[
\|y\|_* = \sup_{\|x\|\le 1}\langle y,x\rangle,
\]
then for the norm $f(x)=\|x\|$ one has
\[
f^*(y)=\begin{cases}
0 & \text{if }\|y\|_\infty\le 1,\\
+\infty & \text{otherwise}
\end{cases}
\]
See apendix to see proof

If we inject our variable we get : 

\[
\inf_{\beta \in \mathbb{R}^p} \left( \lambda \|\beta\|_1 - \nu^T X\beta \right) = -f^*\left(\frac{X^T\nu}{\lambda}\right)=\begin{cases}
0 & \text{if }\|{X^T\nu}\|_\infty\le \lambda,\\
-\infty & \text{otherwise}
\end{cases}
\]

\subsection*{5. The Dual Problem}
The dual problem $(D)$ is to maximize $g(\nu)$. We can write this as a minimization problem by flipping the sign, which is a common convention:
\[
(D) \quad d^* = \max_{\nu \in \mathbb{R}^n} \quad - \frac{1}{2}\|\nu\|_2^2 + y^T\nu \quad \text{subject to} \quad \|X^T \nu\|_\infty \le \lambda
\]

\subsection*{6. Finding $\lambda_{\text{max}}$}
We want to find the value $\lambda_{\text{max}}$ such that for any $\lambda > \lambda_{\text{max}}$, the primal solution is $\beta^* = 0_p$.

If the primal solution is $\beta^* = 0_p$

Now we use the KKT optimality conditions. The stationarity condition (from minimizing the Lagrangian w.r.t. $z$) gave us the relationship:
\[
z^* + \nu^* = 0 \quad \implies \quad \nu^* = -z^*
\]
(Note: Using the other sign convention for the Lagrangian, $\nu^* = z^*$, leads to the same final result).

Substituting our primal solutions, the optimal dual variable must be:
\[
\nu^* = -z^* = -y
\]
For $\beta^* = 0_p$ to be the optimal solution, its corresponding dual partner $\nu^* = -y$ must be a **feasible solution for the dual problem $(D)$**.

The feasibility condition for the dual problem is its constraint:
\[
\|X^T \nu^*\|_\infty \le \lambda
\]
We substitute $\nu^* = -y$ into this constraint:
\[
\|X^T (-y)\|_\infty \le \lambda
\]
Since the $L_\infty$-norm is absolute ($|-a| = |a|$), this is:
\[
\|X^T y\|_\infty \le \lambda
\]
This is the condition on $\lambda$ that ensures the solution is $\beta^* = 0_p$. If $\lambda$ is greater than or equal to this value, the optimal solution will be the zero vector.

Therefore, the threshold $\lambda_{\text{max}}$ is the smallest value for which this holds:
\[
\lambda_{\text{max}} = \|X^T y\|_\infty
\]

This means $\lambda_{\text{max}}$ is the largest (in absolute value) inner product between any single feature and the response vector $y$.

\begin{equation}
    \lambda_{\max} = \dots
\end{equation}

\end{solution}

\begin{exercise}
For a univariate signal $\mathbf{x}\in\mathbb{R}^n$ with $n$ samples, the convolutional dictionary learning task amounts to solving the following optimization problem:

\begin{equation}
\min_{(\mathbf{d}_k)_k, (\mathbf{z}_k)_k \\ \norm{\mathbf{d}_k}_2^2\leq 1} \quad\norm{\mathbf{x} - \sum_{k=1}^K \mathbf{z}_k * \mathbf{d}_k }^2_2 \quad + \quad\lambda \sum_{k=1}^K \norm{\mathbf{z}_k}_1
\end{equation}

where $\mathbf{d}_k\in\mathbb{R}^L$ are the $K$ dictionary atoms (patterns), $\mathbf{z}_k\in\mathbb{R}^{N-L+1}$ are activations signals, and $\lambda>0$ is the smoothing parameter.

Show that
\begin{itemize}
    \item for a fixed dictionary, the sparse coding problem is a lasso regression (explicit the response vector and the design matrix);
    \item for a fixed dictionary, there exists $\lambda_{\max}$ (which depends on the dictionary) such that the sparse codes are only 0 for any $\lambda > \lambda_{\max}$. 
\end{itemize}
\end{exercise}

\begin{solution}  % ANSWER HERE
We fix all the dictionary atoms $\mathbf{d}_k \in \mathbb{R}^{L}$ for $k = 1, \dots, K$,
and we note that $M = N - L + 1$ is the valid convolution length. 

For a given pair $(\mathbf{z}_k, \mathbf{d}_k)$, the valid convolution 
$\mathbf{z}_k * \mathbf{d}_k \in \mathbb{R}^{N}$ can be written componentwise as
\[
(\mathbf{z}_k * \mathbf{d}_k)[n] = \sum_{i=0}^{M-1} z_k[i]\, d_k[n - i],
\quad \text{where } d_k[n - i] = 0 \text{ if } n - i < 0 \text{ or } n - i > L - 1.
\]

This operation can be expressed as a matrix–vector product
\[
\mathbf{y}_k = \mathbf{D}_k \mathbf{z}_k,
\]
where $\mathbf{D}_k \in \mathbb{R}^{N \times M}$ has coefficients
\[
\mathbf{D}_k[n, i] =
\begin{cases}
d_k[n - i], & \text{if } 0 \le n - i < L, \\[6pt]
0, & \text{otherwise.}
\end{cases}
\]

We define:
\[
\mathbf{z} =
\begin{bmatrix}
\mathbf{z}_1 \\ \vdots \\ \mathbf{z}_K
\end{bmatrix} \in \mathbb{R}^{K \cdot M},
\qquad
\mathbf{D} =
\begin{bmatrix}
\mathbf{D}_1 & \mathbf{D}_2 & \cdots & \mathbf{D}_K
\end{bmatrix} \in \mathbb{R}^{N \times K \cdot M}.
\]

Hence we can then rewrite the sparse coding problem given the $(\mathbf{d}_k)$ in the LASSO form, with 
response vector $\mathbf{x} \in \mathbb{R}^{N}$, design matrix $\mathbf{D}$ and regressor vector $\mathbf{z}$:
\[
\min_{\mathbf{z} \in \mathbb{R}^{K \cdot M}} \;
\left\| \mathbf{x} - \mathbf{D} \mathbf{z} \right\|_2^2
\;+\;
\lambda \|\mathbf{z}\|_1.
\]



\end{solution}

\section{Spectral feature}

Let $X_n$ ($n=0,\dots, N-1)$ be a weakly stationary random process with zero mean and autocovariance function $\gamma(\tau):= \mathbb{E}(X_n X_{n+\tau})$.
Assume the autocovariances are absolutely summable, \ie $\sum_{\tau\in\mathbb{Z}} |\gamma(\tau)| < \infty$, and square summable, \ie $\sum_{\tau\in\mathbb{Z}} \gamma^2(\tau) < \infty$.
Denote the sampling frequency by $f_s$, meaning that the index $n$ corresponds to the time $n / f_s$. For simplicity, let $N$ be even.


The \textit{power spectrum} $S$ of the stationary random process $X$ is defined as the Fourier transform of the autocovariance function:
\begin{equation}
    S(f) := \sum_{\tau=-\infty}^{+\infty}\gamma(\tau)e^{-2\pi f\tau/f_s}.
\end{equation}
The power spectrum describes the distribution of power in the frequency space. 
Intuitively, large values of $S(f)$ indicate that the signal contains a sine wave at the frequency $f$.
There are many estimation procedures to determine this important quantity, which can then be used in a machine-learning pipeline.
In the following, we discuss the large sample properties of simple estimation procedures and the relationship between the power spectrum and the autocorrelation.

(Hint: use the many results on quadratic forms of Gaussian random variables to limit the number of calculations.)

\begin{exercise}
In this question, let $X_n$ ($n=0,\dots,N-1)$ be a Gaussian white noise.

\begin{itemize}
    \item Calculate the associated autocovariance function and power spectrum. (By analogy with the light, this process is called ``white'' because of the particular form of its power spectrum.)
\end{itemize}

\end{exercise}

\begin{solution}
Assume $(X_n)$ is Gaussian white noise with variance $\sigma^2$, i.e. the $X_n$ are i.i.d.\ $\mathcal{N}(0,\sigma^2)$.

\subsection*{Autocovariance}
Independence gives for any integer lag $\tau$:
\[
\gamma(\tau)=\mathbb{E}[X_n X_{n+\tau}] =
\begin{cases}
\sigma^2, & \tau=0,\\[4pt]
0, & \tau\neq 0.
\end{cases}
\]
Equivalently $\gamma(\tau)=\sigma^2\mathbf{1}_{\{\tau=0\}}$ (Kronecker delta form).

\subsection*{Power spectrum}
The power spectrum is the Fourier transform of $\gamma(\tau)$:
\[
S(f)=\sum_{\tau=-\infty}^{+\infty}\gamma(\tau)\,e^{-2\pi i f \tau / f_s}
= \sigma^2 \sum_{\tau=-\infty}^{+\infty}\mathbf{1}_{\{\tau=0\}} e^{-2\pi i f \tau / f_s}
= \sigma^2,
\]
i.e. the spectrum is \emph{constant} (flat) across all frequencies.  
This constant spectrum is the reason the process is called ``white'' noise (analogy to white light which has equal power at all visible frequencies).
\end{solution}


\begin{exercise}
A natural estimator for the autocorrelation function is the sample autocovariance
\begin{equation}
    \hat{\gamma}(\tau) := (1/N) \sum_{n=0}^{N-\tau-1} X_n X_{n+\tau}
\end{equation}
for $\tau=0,1,\dots,N-1$ and $\hat{\gamma}(\tau):=\hat{\gamma}(-\tau)$ for $\tau=-(N-1),\dots,-1$.
\begin{itemize}
    \item Show that $\hat{\gamma}(\tau)$ is a biased estimator of $\gamma(\tau)$ but asymptotically unbiased.
    What would be a simple way to de-bias this estimator?
\end{itemize}

\end{exercise}

\begin{solution}
\subsection*{Bias of \(\hat\gamma(\tau)\)}
Compute the expectation (for any stationary process with mean zero):
\[
\mathbb{E}\big[\hat{\gamma}(\tau)\big]
= \frac{1}{N}\sum_{n=0}^{N-\tau-1}\mathbb{E}[X_n X_{n+\tau}]
= \frac{N-\tau}{N}\,\gamma(\tau).
\]
Thus
\[
\mathbb{E}\big[\hat{\gamma}(\tau)\big] = \left(1-\frac{\tau}{N}\right)\gamma(\tau).
\]
Therefore $\hat\gamma(\tau)$ is \emph{biased} for finite $N$ (unless $\gamma(\tau)=0$ or $\tau=0$).  
However, as $N\to\infty$ with fixed $\tau$,
\[
\mathbb{E}\big[\hat{\gamma}(\tau)\big] \longrightarrow \gamma(\tau),
\]
so $\hat\gamma(\tau)$ is \emph{asymptotically unbiased}.

\subsection*{A simple de-biasing (unbiased estimator)}
A straightforward unbiased estimator is obtained by dividing by the number of terms $(N-\tau)$ instead of $N$:
\[
\tilde{\gamma}(\tau):=\frac{1}{N-\tau}\sum_{n=0}^{N-\tau-1} X_n X_{n+\tau}.
\]
Indeed,
\[
\mathbb{E}\big[\tilde{\gamma}(\tau)\big] = \frac{1}{N-\tau}\sum_{n=0}^{N-\tau-1}\gamma(\tau)=\gamma(\tau).
\]
So $\tilde\gamma(\tau)$ is unbiased for every finite $N$ (for $0\le\tau\le N-1$).  
This is the usual ``$N-\tau$'' normalization used when one wants an unbiased sample autocovariance.

\end{solution}

\begin{exercise}
Define the discrete Fourier transform of the random process $\{X_n\}_n$ by
\begin{equation}
    J(f) := (1/\sqrt{N})\sum_{n=0}^{N-1} X_n e^{-2\pi\iu f n/f_s}
\end{equation}
The \textit{periodogram} is the collection of values $|J(f_0)|^2$, $|J(f_1)|^2$, \dots, $|J(f_{N/2})|^2$ where $f_k = f_s k/N$.
(They can be efficiently computed using the Fast Fourier Transform.)
\begin{itemize}
    \item Write $|J(f_k)|^2$ as a function of the sample autocovariances.
    \item For a frequency $f$, define $f^{(N)}$ the closest Fourier frequency $f_k$ to $f$.
    Show that $|J(f^{(N)})|^2$ is an asymptotically unbiased estimator of $S(f)$ for $f>0$.
\end{itemize}
\end{exercise}

\begin{solution}
\[
|J(f_k)|^2 = J(f_k) \overline{J(f_k)} 
= (1/N) \sum_{n=0}^{N-1} \sum_{m=0}^{N-1} X_n X_m e^{-2\pi\iu f_k (n - m)/f_s}
\]

Do the change of variable $\tau = m - n \in [-(N-1), N-1]$.
\[
|J(f_k)|^2 = \frac{1}{N} \sum_{\tau=-(N-1)}^{N-1} 
    e^{-2\pi \mathrm{i} f_k \tau / f_s}
    \sum_{n=0}^{N-1 - |\tau|} X_n X_{n + |\tau|} \\[4pt]
\]

\[
|J(f_k)|^2= \sum_{\tau=-(N-1)}^{N-1} 
    \hat{\gamma}(\tau)\, e^{-2\pi \mathrm{i} f_k \tau / f_s}.
\]

For a frequency $f$, let $f^{(N)}$ be the closest Fourier frequency to $f$,
with $k_n = \lfloor N f / f_s \rfloor$.

\[
\mathbb{E}[|J(f^{(N)})|^2]
= \sum_{\tau=-(N-1)}^{N-1} 
    \mathbb{E}[\hat{\gamma}(\tau)]\, e^{-2\pi \mathrm{i} f^{(N)} \tau / f_s}
\]

\[
\mathbb{E}[|J(f^{(N)})|^2]
= \frac{N-|\tau|}{N} \sum_{\tau=-(N-1)}^{N-1} 
     \gamma(\tau)\, e^{-2\pi \mathrm{i} f^{(N)} \tau / f_s}
\]

We assume the summability of the autocovariances and their square is sufficient
to take the limit of $N$ in the whole expression, to have

\[\boxed{
\mathbb{E}[|J(f^{(N)})|^2] \rightarrow \sum_{\tau=-\infty}^{+\infty} 
     \gamma(\tau)\, e^{-2\pi \mathrm{i} f \tau / f_s}
= S(f).}
\]


\end{solution}

\begin{exercise}\label{ex:wn-exp}
    In this question, let $X_n$ ($n=0,\dots,N-1)$ be a Gaussian white noise with variance $\sigma^2=1$ and set the sampling frequency to $f_s=1$ Hz
    \begin{itemize}
        \item For $N\in\{200, 500, 1000\}$, compute the \textit{sample autocovariances} ($\hat{\gamma}(\tau)$ vs $\tau$) for 100 simulations of $X$.
        Plot the average value as well as the average $\pm$, the standard deviation.
        What do you observe?
        \item For $N\in\{200, 500, 1000\}$, compute the \textit{periodogram} ($|J(f_k)|^2$ vs $f_k$) for 100 simulations of $X$.
        Plot the average value as well as the average $\pm$, the standard deviation.
        What do you observe?
    \end{itemize}
    Add your plots to Figure~\ref{fig:wn-exp}.
    
\begin{figure}
    \centering
    \begin{minipage}[t]{0.3\textwidth}
    \centerline{\includegraphics[width=\textwidth]{figures/acov_white_noise_N200_fs1.0_sim100.png}}
    \centerline{Autocovariance ($N=200$)}
    \end{minipage}
    \begin{minipage}[t]{0.3\textwidth}
    \centerline{\includegraphics[width=\textwidth]{figures/acov_white_noise_N500_fs1.0_sim100.png}}
    \centerline{Autocovariance ($N=500$)}
    \end{minipage}
    \begin{minipage}[t]{0.3\textwidth}
    \centerline{\includegraphics[width=\textwidth]{figures/acov_white_noise_N1000_fs1.0_sim100.png}}
    \centerline{Autocovariance ($N=1000$)}
    \end{minipage}
    \vskip1em
    \begin{minipage}[t]{0.3\textwidth}
    \centerline{\includegraphics[width=\textwidth]{figures/periodogram_white_noise_N200_fs1.0_sim100.png}}
    \centerline{Periodogram ($N=200$)}
    \end{minipage}
    \begin{minipage}[t]{0.3\textwidth}
    \centerline{\includegraphics[width=\textwidth]{figures/periodogram_white_noise_N500_fs1.0_sim100.png}}
    \centerline{Periodogram ($N=500$)}
    \end{minipage}
    \begin{minipage}[t]{0.3\textwidth}
    \centerline{\includegraphics[width=\textwidth]{figures/periodogram_white_noise_N1000_fs1.0_sim100.png}}
    \centerline{Periodogram ($N=1000$)}
    \end{minipage}
    \vskip1em
    \caption{Autocovariances and periodograms of a Gaussian white noise (see Question~\ref{ex:wn-exp}).}
    \label{fig:wn-exp}
\end{figure}

\end{exercise}

\begin{solution}
    \begin{itemize}
        \item The theoretical autocovariance of white noise is $\sigma^2$ at lag=0 and 0 everywhere else. 
        Here the sample autocovariance has the peak of 1 at lag=0 and abruptly decreases to low fluctuations around 0 with standard deviation of ~0.1 for N=200.
        The standard deviation slowly decreases with the number of lags. That is not obvious from the expression derived further in Question 7. With bigger N, the standard deviation is lower for all lags, around 0.02 for N=500 at low lag.
        \item Theoretically, the periodogram of gaussian white noise is flat and equal to $\sigma^2/f_s = 1$ at all frequencies.
        Here we have a mean around 1 and a standard deviation of 1 too (we rescaled the autocovariances by 0.5 after computing the onesided periodogram)
        The standard deviation does not decrease with N.
    \end{itemize}

\end{solution}

\begin{exercise}
    We want to show that the estimator $\hat{\gamma}(\tau)$ is consistent, \ie it converges in probability when the number $N$ of samples grows to $\infty$ to the true value ${\gamma}(\tau)$.
    In this question, assume that $X$ is a wide-sense stationary \textit{Gaussian} process.
    \begin{itemize}
        \item Show that for $\tau>0$ 
    \begin{equation}
       \text{var}(\hat{\gamma}(\tau)) = (1/N) \sum_{n=-(N-\tau-1)}^{n=N-\tau-1} \left(1 - \frac{\tau + |n|}{N}\right) \left[\gamma^2(n) + \gamma(n-\tau)\gamma(n+\tau)\right].
    \end{equation}
    (Hint: if $\{Y_1, Y_2, Y_3, Y_4\}$ are four centered jointly Gaussian variables, then $\mathbb{E}[Y_1 Y_2 Y_3 Y_4] = \mathbb{E}[Y_1 Y_2]\mathbb{E}[Y_3 Y_4] + \mathbb{E}[Y_1 Y_3]\mathbb{E}[Y_2 Y_4] + \mathbb{E}[Y_1 Y_4]\mathbb{E}[Y_2 Y_3]$.) 
    \item Conclude that $\hat{\gamma}(\tau)$ is consistent.
    \end{itemize}
\end{exercise}

\begin{solution}
This solution builds upon the results from Question 4. We assume $X_n$ is a wide-sense stationary (WSS) Gaussian process with $\mathbb{E}[X_n] = 0$ (centered). The true autocovariance is $\gamma(\tau) = \mathbb{E}[X_n X_{n+\tau}]$. The estimator is $\hat{\gamma}(\tau) = \frac{1}{N} \sum_{n=0}^{N-\tau-1} X_n X_{n+\tau}$.

\subsection*{Part 1: Deriving the Variance}

The variance of the estimator is defined as:
\[
\text{var}(\hat{\gamma}(\tau)) = \mathbb{E}[\hat{\gamma}(\tau)^2] - \left(\mathbb{E}[\hat{\gamma}(\tau)]\right)^2
\]
From Question 4, we know the second term: $\mathbb{E}[\hat{\gamma}(\tau)] = \left(1 - \frac{\tau}{N}\right) \gamma(\tau)$.
So, $\left(\mathbb{E}[\hat{\gamma}(\tau)]\right)^2 = \left(1 - \frac{\tau}{N}\right)^2 \gamma^2(\tau)$.

The main task is to compute the first term, $\mathbb{E}[\hat{\gamma}(\tau)^2]$.
\[
\mathbb{E}[\hat{\gamma}(\tau)^2] = \mathbb{E}\left[ \left( \frac{1}{N} \sum_{n=0}^{N-\tau-1} X_n X_{n+\tau} \right) \left( \frac{1}{N} \sum_{m=0}^{N-\tau-1} X_m X_{m+\tau} \right) \right]
\]
\[
\mathbb{E}[\hat{\gamma}(\tau)^2] = \frac{1}{N^2} \sum_{n=0}^{N-\tau-1} \sum_{m=0}^{N-\tau-1} \mathbb{E}[X_n X_{n+\tau} X_m X_{m+\tau}]
\]
Now we use the hint for the expectation of four centered Gaussian variables. Let $Y_1=X_n, Y_2=X_{n+\tau}, Y_3=X_m, Y_4=X_{m+\tau}$.
\[
\mathbb{E}[Y_1 Y_2 Y_3 Y_4] = \mathbb{E}[Y_1 Y_2]\mathbb{E}[Y_3 Y_4] + \mathbb{E}[Y_1 Y_3]\mathbb{E}[Y_2 Y_4] + \mathbb{E}[Y_1 Y_4]\mathbb{E}[Y_2 Y_3]
\]
Let's translate this back using the definition $\gamma(k) = \mathbb{E}[X_i X_{i+k}]$.
\begin{itemize}
    \item $\mathbb{E}[Y_1 Y_2]\mathbb{E}[Y_3 Y_4] = \mathbb{E}[X_n X_{n+\tau}] \mathbb{E}[X_m X_{m+\tau}] = \gamma(\tau) \cdot \gamma(\tau) = \gamma^2(\tau)$
    \item $\mathbb{E}[Y_1 Y_3]\mathbb{E}[Y_2 Y_4] = \mathbb{E}[X_n X_m] \mathbb{E}[X_{n+\tau} X_{m+\tau}] = \gamma(m-n) \cdot \gamma((m+\tau)-(n+\tau)) = \gamma(m-n) \gamma(m-n) = \gamma^2(m-n)$
    \item $\mathbb{E}[Y_1 Y_4]\mathbb{E}[Y_2 Y_3] = \mathbb{E}[X_n X_{m+\tau}] \mathbb{E}[X_{n+\tau} X_m] = \gamma(m+\tau-n) \cdot \gamma(m-(n+\tau)) = \gamma(m-n+\tau)\gamma(m-n-\tau)$
\end{itemize}
Substituting this into the double sum for $\mathbb{E}[\hat{\gamma}(\tau)^2]$:
\[
\mathbb{E}[\hat{\gamma}(\tau)^2] = \frac{1}{N^2} \sum_{n=0}^{N-\tau-1} \sum_{m=0}^{N-\tau-1} \left[ \gamma^2(\tau) + \gamma^2(m-n) + \gamma(m-n+\tau)\gamma(m-n-\tau) \right]
\]
We separate the $\gamma^2(\tau)$ term, which is constant in the sums:
\[
\frac{1}{N^2} \sum_{n=0}^{N-\tau-1} \sum_{m=0}^{N-\tau-1} \gamma^2(\tau) = \frac{1}{N^2} (N-\tau)(N-\tau) \gamma^2(\tau) = \left(\frac{N-\tau}{N}\right)^2 \gamma^2(\tau) = \left(1 - \frac{\tau}{N}\right)^2 \gamma^2(\tau)
\]
This is exactly the term $\left(\mathbb{E}[\hat{\gamma}(\tau)]\right)^2$.

Now, let's find the variance:
\[
\text{var}(\hat{\gamma}(\tau)) = \mathbb{E}[\hat{\gamma}(\tau)^2] - \left(\mathbb{E}[\hat{\gamma}(\tau)]\right)^2
\]
\[
\text{var}(\hat{\gamma}(\tau)) = \left[ \left(1 - \frac{\tau}{N}\right)^2 \gamma^2(\tau) + \dots \right] - \left(1 - \frac{\tau}{N}\right)^2 \gamma^2(\tau)
\]
The $\gamma^2(\tau)$ terms cancel out, leaving only the other two terms from the Gaussian expansion:
\[
\text{var}(\hat{\gamma}(\tau)) = \frac{1}{N^2} \sum_{n=0}^{N-\tau-1} \sum_{m=0}^{N-\tau-1} \left[ \gamma^2(m-n) + \gamma(m-n+\tau)\gamma(m-n-\tau) \right]
\]
This is a double sum where the summand only depends on the difference $k = m-n$. We can convert this to a single sum over $k$.
Let $A = N-\tau$. The sum is $\sum_{n=0}^{A-1} \sum_{m=0}^{A-1} f(m-n)$. The lag $k = m-n$ ranges from $-(A-1)$ to $A-1$.
For any given $k$ in this range, the number of $(n, m)$ pairs that produce this lag is $(A - |k|)$, or $(N-\tau-|k|)$.
So, we can rewrite the double sum as:
\[
\text{var}(\hat{\gamma}(\tau)) = \frac{1}{N^2} \sum_{k=-(N-\tau-1)}^{N-\tau-1} (N-\tau-|k|) \left[ \gamma^2(k) + \gamma(k+\tau)\gamma(k-\tau) \right]
\]
To match the formula in the exercise, we factor out $1/N$:
\[
\text{var}(\hat{\gamma}(\tau)) = \frac{1}{N} \sum_{k=-(N-\tau-1)}^{N-\tau-1} \frac{N-\tau-|k|}{N} \left[ \gamma^2(k) + \gamma(k+\tau)\gamma(k-\tau) \right]
\]
\[
\text{var}(\hat{\gamma}(\tau)) = \frac{1}{N} \sum_{k=-(N-\tau-1)}^{N-\tau-1} \left(1 - \frac{\tau+|k|}{N}\right) \left[ \gamma^2(k) + \gamma(k+\tau)\gamma(k-\tau) \right]
\]
This completes the proof of the first part.

\subsection*{Part 2: Concluding on Consistency}
An estimator $\hat{\theta}_N$ is consistent for $\theta$ if it converges in probability to $\theta$ as $N \to \infty$. That is, for any $\epsilon > 0$:
\[
\lim_{N \to \infty} P\left(|\hat{\gamma}(\tau) - \gamma(\tau)| \ge \epsilon\right) = 0
\]
We can prove this using Chebyshev's inequality. A general form of Chebyshev's inequality (sometimes called the "MSE" form or derived from Markov's inequality) states:
\[
P\left(|\hat{\gamma}(\tau) - \gamma(\tau)| \ge \epsilon\right) \le \frac{\mathbb{E}\left[(\hat{\gamma}(\tau) - \gamma(\tau))^2\right]}{\epsilon^2}
\]
The term in the numerator is the **Mean Squared Error (MSE)** of the estimator.
\[
\text{MSE}(\hat{\gamma}(\tau)) = \mathbb{E}\left[(\hat{\gamma}(\tau) - \gamma(\tau))^2\right] = \text{Bias}(\hat{\gamma}(\tau))^2 + \text{var}(\hat{\gamma}(\tau))
\]
To prove consistency, we just need to show that the MSE tends to zero as $N \to \infty$.

\textbf{1. Bias Term:}
From Question 4, we know the estimator is asymptotically unbiased. The bias is:
\[
\text{Bias}(\hat{\gamma}(\tau)) = \mathbb{E}[\hat{\gamma}(\tau)] - \gamma(\tau) = \left(1 - \frac{\tau}{N}\right) \gamma(\tau) - \gamma(\tau) = -\frac{\tau}{N} \gamma(\tau)
\]
As $N \to \infty$, the bias clearly goes to zero:
\[
\lim_{N \to \infty} \text{Bias}(\hat{\gamma}(\tau)) = \lim_{N \to \infty} -\frac{\tau}{N} \gamma(\tau) = 0
\]
Therefore, $\lim_{N \to \infty} \text{Bias}(\hat{\gamma}(\tau))^2 = 0$.

\textbf{2. Variance Term:}
From Part 1 of this exercise, we must show that $\lim_{N \to \infty} \text{var}(\hat{\gamma}(\tau)) = 0$.
\[
\text{var}(\hat{\gamma}(\tau)) = \frac{1}{N} \sum_{k=-(N-\tau-1)}^{N-\tau-1} \underbrace{\left(1 - \frac{\tau+|k|}{N}\right)}_{\le 1} \left[ \gamma^2(k) + \gamma(k+\tau)\gamma(k-\tau) \right]
\]
The term $\left(1 - \frac{\tau+|k|}{N}\right)$ is always positive and less than or equal to 1.
\[
\text{var}(\hat{\gamma}(\tau)) \le \frac{1}{N} \sum_{k=-(N-\tau-1)}^{N-\tau-1} \left| \gamma^2(k) + \gamma(k+\tau)\gamma(k-\tau) \right|
\]
We assumed that the autocovariances are absolutely summable, i.e., $\sum_{k=-\infty}^{\infty} |\gamma(k)| < \infty$. This implies $\gamma(k) \to 0$ as $|k| \to \infty$.
If this holds, then $\gamma^2(k)$ and products like $\gamma(k+\tau)\gamma(k-\tau)$ are also absolutely summable.
This means the sum converges to a finite constant $C$ as $N \to \infty$:
\[
\sum_{k=-\infty}^{\infty} \left| \gamma^2(k) + \gamma(k+\tau)\gamma(k-\tau) \right| = C < \infty
\]
Therefore,
\[
0 \le \lim_{N \to \infty} \text{var}(\hat{\gamma}(\tau)) \le \lim_{N \to \infty} \frac{1}{N} \sum_{k=-(N-\tau-1)}^{N-\tau-1} \left| \dots \right| = \lim_{N \to \infty} \frac{C}{N} = 0
\]
By the squeeze theorem, $\lim_{N \to \infty} \text{var}(\hat{\gamma}(\tau)) = 0$.

\textbf{Conclusion:}
Since $\lim_{N \to \infty} \text{Bias}(\hat{\gamma}(\tau))^2 = 0$ and $\lim_{N \to \infty} \text{var}(\hat{\gamma}(\tau)) = 0$, we have:
\[
\lim_{N \to \infty} \text{MSE}(\hat{\gamma}(\tau)) = 0
\]
Plugging this into our Chebyshev inequality:
\[
\lim_{N \to \infty} P\left(|\hat{\gamma}(\tau) - \gamma(\tau)| \ge \epsilon\right) \le \lim_{N \to \infty} \frac{\text{MSE}(\hat{\gamma}(\tau))}{\epsilon^2} = \frac{0}{\epsilon^2} = 0
\]
Since the probability is non-negative, by the squeeze theorem, the limit must be 0.
This proves that $\hat{\gamma}(\tau)$ converges in probability to $\gamma(\tau)$ and is therefore a consistent estimator.

\end{solution}
\newpage
Contrary to the correlogram, the periodogram is not consistent.
It is one of the most well-known estimators that is asymptotically unbiased but not consistent.
In the following question, this is proven for Gaussian white noise, but this holds for more general stationary processes.
\begin{exercise}
    Assume that $X$ is a Gaussian white noise (variance $\sigma^2$) and let $A(f):=\sum_{n=0}^{N-1} X_n \cos(-2\pi f n/f_s)$ and $B(f):=\sum_{n=0}^{N-1} X_n \sin(-2\pi f n/f_s$.
    Observe that $J(f) = (1/N) (A(f) + \iu B(f))$.
    \begin{itemize}
        \item Derive the mean and variance of $A(f)$ and $B(f)$ for $f=f_0, f_1,\dots, f_{N/2}$ where $f_k=f_s k/N$.
        \item What is the distribution of the periodogram values $|J(f_0)|^2$, $|J(f_1)|^2$, \dots, $|J(f_{N/2})|^2$.
        \item What is the variance of the $|J(f_k)|^2$? Conclude that the periodogram is not consistent.
        \item Explain the erratic behavior of the periodogram in Question~\ref{ex:wn-exp} by looking at the covariance between the $|J(f_k)|^2$.
    \end{itemize}
\end{exercise}

\begin{solution}

There is a small typo : $J(f) = (1/\sqrt{N}) (A(f) + \iu B(f))$

1) $A(f)$ and $B(f)$ are linear combinations of centered independent Gaussian variables, hence
they are also centered Gaussian:
\[
\mathbb{E}[A(f)] = 0, \quad \mathbb{E}[B(f)] = 0.
\]

For the variance, by bilinearity of covariances:
\begin{align*}
\mathrm{var}\big(A(f)\big) 
&= \mathrm{cov}\!\left( 
    \sum_{n=0}^{N-1} X_n \cos(2\pi f n/f_s),
    \sum_{m=0}^{N-1} X_m \cos(2\pi f m/f_s)
\right) \\
&= \sum_{n=0}^{N-1} \sum_{m=0}^{N-1} 
    \mathrm{cov}(X_n, X_m)\,
    \cos(2\pi f n/f_s)
    \cos(2\pi f m/f_s) \\
&= \sigma^2 \sum_{n=0}^{N-1} 
    \cos^2(2\pi f n/f_s).
\end{align*}

For $k \in \{1, \ldots, N/2-1\}$, we have:
\begin{align*}
\mathrm{var}\big(A(f_k)\big) = \sigma^2 \sum_{n=0}^{N-1} 
    \cos^2\!\left(\frac{2\pi k n}{N}\right) = \sigma^2 \sum_{n=0}^{N-1} 
    \frac{1}{2}\left(1 + \cos\!\left(\frac{4\pi k n}{N}\right)\right) = \frac{N\sigma^2}{2}.
\end{align*}


Because for $k \neq 0$ and $k \neq N/2$, the sum of cosines is zero.
The same result holds for $B(f_k)$.

For $k = 0$:  $\cos^2(0) = 1$ and $\sin^2(0) = 0$. For $k = N/2$: $\cos^2(\pi n) = 1$ and $\sin^2(\pi n) = 0$,

To sum up:
\[
  \boxed{
  \operatorname{var}\big(A(f_k)\big)
  = \operatorname{var}\big(B(f_k)\big)
  = \tfrac{N}{2}\sigma^2.}
\]
\[
  \boxed{
  \operatorname{var}\big(A(f_0)\big) = N\sigma^2, \quad
  \operatorname{var}\big(B(f_0)\big) = 0.}
\]
\[
  \boxed{
  \operatorname{var}\big(A(f_{N/2})\big) = N\sigma^2, \quad
  \operatorname{var}\big(B(f_{N/2})\big) = 0.}
  \]


2) The periodogram writes:
\[
|J(f_k)|^2 = \frac{1}{N} \big(A(f_k)^2 + B(f_k)^2\big).
\]

Define the normalized variables $Z_A$ and $Z_B$ such that:
\[A(f_k) = \sqrt{(N/2)\sigma^2} Z_A(f_k) , \quad B(f_k) = \sqrt{(N/2)\sigma^2} Z_B(f_k).\]

For $0 < k < N/2$:
\begin{align*}
|J(f_k)|^2
    = \frac{1}{N} \left((N/2)\sigma^2 Z_A^2 + (N/2)\sigma^2 Z_B^2\right)
    = \frac{\sigma^2}{2} (Z_A^2 + Z_B^2),
\end{align*}

For $k = 0$ or $k = N/2$, we have $B(f_k) = 0$ and 
$A(f_k) \sim \mathcal{N}(0, N\sigma^2)$, hence
\[
|J(f_k)|^2 = \frac{1}{N} A(f_k)^2 = \frac{1}{N} N\sigma^2 Z_A^2 = \sigma^2 Z_A^2.
\]

And using $2 \cos(x) \sin(y) = \sin(2x)$, we have:

\begin{align*}
\mathrm{cov}(A(f_k), B(f_k)) 
&= \sum_{n=0}^{N-1} \sum_{m=0}^{N-1} 
    \mathrm{cov}(X_n, X_m)\,
    \cos(2\pi f_k n/f_s)
    \sin(-2\pi f_k m/f_s) \\
&= \sigma^2 \sum_{n=0}^{N-1}
    \frac{1}{2}\sin(-4\pi f_k n/f_s) = 0.
\end{align*}

Hence $A(f_k)$, $B(f_k)$ are uncorrelated Gaussian variables, hence independent.
Same for $Z_A$ and $Z_B$.
And the periodogram is then a sum of squares of two independent standard normal variables,
that is it follows a $\chi^2$ distribution:

For $0 < k < N/2$:
\[\boxed{
|J(f_k)|^2 = \frac{\sigma^2}{2}(Z_A^2 + Z_B^2)
    \sim \frac{\sigma^2}{2}\chi^2_2.}
\]

For $k = 0$ or $k = N/2$:
\[\boxed{
|J(f_k)|^2 = \sigma^2 Z_A^2
    \sim \sigma^2\chi^2_1.}
\]




3) For $0 < k < N/2$:
\begin{align*}\boxed{
\operatorname{Var}[|J(f_k)|^2] 
= \operatorname{Var}\!\left[\frac{\sigma^2}{2}\chi^2_2\right] 
= \frac{\sigma^4}{4} \operatorname{Var}[\chi^2_2]
= \frac{\sigma^4}{4} \cdot (2 \cdot 2) = \sigma^4}
\end{align*}

And for $k = 0$ or $k = N/2$:
\begin{align*}\boxed{
\operatorname{Var}[|J(f_k)|^2]
= \operatorname{Var}[\sigma^2 \chi^2_1]
= \sigma^4 \operatorname{Var}[\chi^2_1]
= \sigma^4 \cdot 2 = 2\sigma^4}
\end{align*}

For all $k$, the variance does not depend on $N$ and does not go to $0$ as $N \to \infty$. Without
providing a strong argument of why it disproves consistency, we feel it must be the case and conclude that the periodogram is not consistent.
Variance is a lower bound on mean squared error, but logical implication between MSE not converging to $0$ and no consistency is not straightforward.

4)
The covariance between two periodogram values at different frequencies $f_k$ and $f_{l}$, with $k \neq l$, is $0$.
We don't prove it but it is similar calculation as for $A(f_k)$ and $B(f_k)$ not correlated and independent as gaussian,
but here we take $A^2(f_k)$, $B^2(f_k)$, $A^2(f_l)$ and $B^2(f_l)$.
Hence the periodogram value at one frequency is completely not correlated with the value at the next frequency.
It explains the erratic behavior and noisy and spiky look, rather than a smooth curve.




\end{solution}


\begin{exercise}\label{q:barlett}
    As seen in the previous question, the problem with the periodogram is the fact that its variance does not decrease with the sample size.
    A simple procedure to obtain a consistent estimate is to divide the signal into $K$ sections of equal durations, compute a periodogram on each section, and average them.
    Provided the sections are independent, this has the effect of dividing the variance by $K$. 
    This procedure is known as Bartlett's procedure.
    \begin{itemize}
        \item Rerun the experiment of Question~\ref{ex:wn-exp}, but replace the periodogram by Barlett's estimate (set $K=5$). What do you observe?
    \end{itemize}
    Add your plots to Figure~\ref{fig:barlett}.
\end{exercise}

\begin{solution}
The Bartlett method to compute periodogram reduces the variance by $1/K$:
\[\operatorname{Var}[\hat{S}_\text{Bartlett}(f_k)] = \frac{1}{K} \operatorname{Var}[\hat{S}(f_k)]\]
As the periodograms on the non overlapping sections are independent. 
Hence it reduces the standard deviation by $1/\sqrt{K} \sim 0.45$, as we see in Figure~\ref{fig:barlett},

\begin{figure}
    \centering
    \begin{minipage}[t]{0.3\textwidth}
    \centerline{\includegraphics[width=\textwidth]{figures/bartlett_periodogram_white_noise_N200_K5_fs1.0_sim100.png}}
    \centerline{Periodogram ($N=200$)}
    \end{minipage}
    \begin{minipage}[t]{0.3\textwidth}
    \centerline{\includegraphics[width=\textwidth]{figures/bartlett_periodogram_white_noise_N500_K5_fs1.0_sim100.png}}
    \centerline{Periodogram ($N=500$)}
    \end{minipage}
    \begin{minipage}[t]{0.3\textwidth}
    \centerline{\includegraphics[width=\textwidth]{figures/bartlett_periodogram_white_noise_N1000_K5_fs1.0_sim100.png}}
    \centerline{Periodogram ($N=1000$)}
    \end{minipage}
    \vskip1em
    \caption{Barlett's periodograms of a Gaussian white noise (see Question~\ref{q:barlett}).}
    \label{fig:barlett}
\end{figure}

\end{solution}
\section{Data study}

\subsection{General information}

\paragraph{Context.}
The study of human gait is a central problem in medical research with far-reaching consequences in the public health domain. This complex mechanism can be altered by a wide range of pathologies (such as Parkinson's disease, arthritis, stroke,\ldots), often resulting in a significant loss of autonomy and an increased risk of falls. Understanding the influence of such medical disorders on a subject's gait would greatly facilitate early detection and prevention of those possibly harmful situations. To address these issues, clinical and bio-mechanical researchers have worked to objectively quantify gait characteristics.

Among the gait features that have proved their relevance in a medical context, several are linked to the notion of step (step duration, variation in step length, etc.), which can be seen as the core atom of the locomotion process. Many algorithms have, therefore, been developed to automatically (or semi-automatically) detect gait events (such as heel-strikes, heel-off, etc.) from accelerometer and gyrometer signals.

\paragraph{Data.}
Data are described in the associated notebook.

\subsection{Step classification with the dynamic time warping (DTW) distance}

\paragraph{Task.} The objective is to classify footsteps and then walk signals between healthy and non-healthy.

\paragraph{Performance metric.} The performance of this binary classification task is measured by the F-score.


\begin{exercise}
Combine the DTW and a k-neighbors classifier to classify each step. Find the optimal number of neighbors with 5-fold cross-validation and report the optimal number of neighbors and the associated F-score. Comment briefly.
\end{exercise}

\begin{solution}
To ensure that the Dynamic Time Warping distance is meaningful and comparable across steps, we pre-processed every step segment before classification. First, a 15 Hz low-pass Butterworth filter was applied to remove high frequency sensor noise not related to gait biomechanics. Then each segment was centered and normalized to unit variance to remove subject-specific amplitude scaling and sensor placement effects. Finally, all steps were resampled to the mean step length of the training set so that the DTW alignment cost was not biased by different segment lengths. This standardization procedure stabilizes DTW, reduces variance, and isolates the temporal morphology of the step, which is the discriminative information we want to classify and not the raw sampling artifacts.

After this preprocessing pipeline, cross-validation on the training set gave an optimal 
$K=7$ for kNN with an F1 score of 0.884 (balanced train). However, on the held-out test set the F1 dropped to 0.523. The drop is explained primarily by the strong class prior shift between train (ratio 0.91) and test (ratio 4.87). The ration being the number of non-healthy/healthy in a dataset. kNN + DTW is a density based classifier, therefore very sensitive to class frequency changes. The method itself generalizes, but its statistical assumptions (balanced priors) changed substantially at test time.
\end{solution}

\newpage
\begin{exercise}\label{q:class-errors}
Display on Figure~\ref{fig:class-errors} a badly classified step from each class (healthy/non-healthy).
\end{exercise}

\begin{solution}
\begin{figure}
    \centering
    \begin{minipage}[t]{\textwidth}
    \centerline{\includegraphics[width=0.6\textwidth]{figures/badly_classified_healthy_step.png}}
    \centerline{Badly classified healthy step}
    \end{minipage}
    \vskip1em
    \begin{minipage}[t]{\textwidth}
    \centerline{\includegraphics[width=0.6\textwidth]{figures/badly_classified_non_healthy_step.png}}
    \centerline{Badly classified non-healthy step}
    \end{minipage}
    \vskip1em
    \caption{Examples of badly classified steps (see Question~\ref{q:class-errors}).}
    \label{fig:class-errors}
\end{figure}
\end{solution}

\newpage
\appendix
\begin{center}
{\Huge Annex}  % or Annexes
\end{center}

\vspace{2em}
\section{Conjugate of the vector $\ell_1$ norm}
Let $f(x)=\|x\|_1$. Compute
\[
f^*(y)=\sup_{x\in\mathbb{R}^n}\{\langle y,x\rangle - \|x\|_1\}.
\]

\subsection*{Case 1: $\|y\|_\infty \le 1$}
Using the Hölder inequality (or the definition of the dual norm),
\[
\langle y,x\rangle \le \|y\|_\infty \, \|x\|_1 \le \|x\|_1.
\]
Thus for every $x$,
\[
\langle y,x\rangle - \|x\|_1 \le 0.
\]
Taking the supremum over $x$ yields $f^*(y)\le 0$. The value $0$ is attained in the limit at $x=0$ (indeed $\langle y,0\rangle-\|0\|_1=0$), hence
\[
f^*(y)=0 \quad\text{when }\|y\|_\infty\le 1.
\]

\subsection*{Case 2: $\|y\|_\infty > 1$}
There exists an index $i$ with $|y_i|>1$. Fix that index and consider vectors of the form $x = t e_i \cdot \operatorname{sign}(y_i)$ for scalar $t>0$, where $e_i$ is the $i$th coordinate unit vector. Then
\[
\langle y,x\rangle - \|x\|_1 = t |y_i| - t = t(|y_i|-1).
\]
Since $|y_i|-1>0$, letting $t\to +\infty$ makes the expression tend to $+\infty$. Therefore $f^*(y)=+\infty$ for any $y$ with $\|y\|_\infty>1$.

\subsection*{Conclusion for vectors}
Combining the two cases,
\[
\boxed{ \; (\|\cdot\|_1)^*(y) =
\begin{cases}
0, & \|y\|_\infty \le 1,\\[4pt]
+\infty, & \|y\|_\infty > 1,
\end{cases} \; }
\]
i.e. the conjugate is the \emph{indicator function} of the $\ell_\infty$ unit ball:
\[
(\|\cdot\|_1)^*(y)=\iota_{\{y:\|y\|_\infty\le 1\}}(y).
\]
This matches the general fact that the conjugate of a (nonnegative, proper) norm is the indicator of the unit ball of its dual norm.

\end{document}
